import os
import glob
import yaml
from pathlib import Path


workdir: "."

base_dir = '/Users/jingfengsun/Documents/model/models/src'

CONFIG_FILE = "configs/qwen2.yaml"


configfile: CONFIG_FILE


MODEL_NAME = config["model"]["name"]
MODEL_BASE_URL = config["model"]["base_url"]
MODEL_API_KEY = config["model"]["api_key"]


# Rule to aggregate all pope_type outputs
rule all:
    input:
        pope_evaluation = config['evaluation']['pope']['flag'],


rule set_local_model:
    input:
        # Load the model configuration from the YAML file
        {CONFIG_FILE},
    output:
        ".deepeval",
    shell:
        """ 
      deepeval set-local-model --model-name='{MODEL_NAME}' \
      --base-url='{MODEL_BASE_URL}' \
      --api-key='{MODEL_API_KEY}' 
      """


rule pope_evaluation:
    input:
        setup=".deepeval",
        local_pope_path=config["evaluation"]["pope"]["params"]["local_pope_path"],
        script=config["evaluation"]["pope"]["script"],
    output:
        pope_evaluation = config['evaluation']['pope']['flag'],
    params:
        remote_coco_path=config["evaluation"]["pope"]["params"]["remote_coco_path"],
        output_dir=os.path.join(
            base_dir,
            "results",
            MODEL_NAME,
            config["evaluation"]["pope"]["params"]["results_folder"],
        ),
        golden_dir= os.path.join(
            base_dir,
            "results",
            MODEL_NAME,
            config["evaluation"]["pope"]["params"]["golden_save_path"],
        ),
        pope_type=config["evaluation"]["pope"]["params"]["pope_types"],
    shell:
        """
        python {input.script} --model_name {MODEL_NAME} \
        --local_pope_path {input.local_pope_path} \
        --remote_coco_path {params.remote_coco_path} \
        --pope_type {params.pope_type} \
        --golden_save_path {params.golden_dir} \
        --results_folder {params.output_dir} \
        --api_base {MODEL_BASE_URL}
        touch {output.pope_evaluation}
        """
rule mm_vet_evaluation:
    input:
        setup='.deepeval',
        script=config["evaluation"]["mm_vet"]["script"],
    output:
        mm_vet_evaluation = config['evaluation']['mm_vet']['flag'],
    params:
        output_dir = os.path.join(
            base_dir,
            "results",
            MODEL_NAME,
            config["evaluation"]["mm_vet"]["params"]["results_folder"],
        ),
        golden_dir = os.path.join(
            base_dir,
            "results",
            MODEL_NAME,
            config["evaluation"]["mm_vet"]["params"]["golden_save_path"],
        ),
        dataset = config["evaluation"]["mm_vet"]["params"]["dataset"],
        dataset_url = config["evaluation"]["mm_vet"]["params"]["dataset_url"],
    shell:
        """
        python {input.script} --model_name {MODEL_NAME} \
        --dataset {params.dataset} \
        --dataset_url {params.dataset_url} \
        --golden_save_path {params.golden_dir} \
        --results_folder {params.output_dir} \
        --api_base {MODEL_BASE_URL}
        touch {output.mm_vet_evaluation}
        """

rule additional_evaluation:
    input:
        setup='.deepeval',
        script=config["evaluation"]["additional"]["script"],
    output:
        additional_evaluation = config['evaluation']['additional']['flag'],
    params:
        output_dir = os.path.join(
            base_dir,
            "results",
            MODEL_NAME,
            config["evaluation"]["additional"]["params"]["results_folder"],
        ),
        remote_data_path = config["evaluation"]["additional"]["params"]["remote_data_path"],
    shell:
        """
        python {input.script} --model_name {MODEL_NAME} \
        --remote_data_path {params.remote_data_path} \
        --results_folder {params.output_dir} \
        --api_base {MODEL_BASE_URL}
        touch {output.additional_evaluation}
        """